{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642eb0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in ./opt/anaconda3/lib/python3.9/site-packages (0.0.2.post1)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (0.1.11)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (1.21.5)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.6.1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (1.8.0)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai)\n",
      "  Using cached tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (6.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (3.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (0.0.81)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in ./opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in ./opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./opt/anaconda3/lib/python3.9/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: idna>=2.8 in ./opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain-openai) (3.3)\n",
      "Requirement already satisfied: certifi in ./opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in ./opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.7->langchain-openai) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (1.26.11)\n",
      "Using cached tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: tiktoken\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.1\n",
      "    Uninstalling tiktoken-0.5.1:\n",
      "      Successfully uninstalled tiktoken-0.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tavily-python 0.3.0 requires tiktoken==0.5.1, but you have tiktoken 0.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67eae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic==1.10.8 in ./opt/anaconda3/lib/python3.9/site-packages (1.10.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from pydantic==1.10.8) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6b5e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typer==0.9.0 in ./opt/anaconda3/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from typer==0.9.0) (8.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./opt/anaconda3/lib/python3.9/site-packages (from typer==0.9.0) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install typer==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88afd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\".....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b89e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can generate test cases and scripts automatically. It can analyze the codebase and create tests based on various inputs, code paths, and edge cases. This helps in increasing test coverage and efficiency.\\n\\n2. Test Data Generation: Langsmith can generate realistic and diverse test data, including different types of inputs, random values, and boundary conditions. This enables thorough testing of the system's behavior under various scenarios.\\n\\n3. Test Oracles: Langsmith can assist in creating test oracles, which are mechanisms to determine whether a test has passed or failed. It can analyze the expected behavior of the system and automatically generate assertions or comparison functions for validating the output.\\n\\n4. Mutation Testing: Langsmith can perform mutation testing by introducing faults or mutations into the codebase. It can create modified versions of the code by altering statements, operators, or variables. This helps in evaluating the effectiveness of the test suite by checking if the mutations are detected.\\n\\n5. Test Optimization: Langsmith can optimize the test suite by identifying redundant or ineffective tests. It can analyze the code coverage achieved by each test and suggest improvements, such as removing duplicate tests or adding missing ones.\\n\\n6. Test Prioritization: Langsmith can prioritize the execution order of tests based on factors like code changes, test dependencies, and historical test results. This maximizes the efficiency of testing by running critical or high-risk tests first.\\n\\n7. Test Maintenance: Langsmith can assist in maintaining the test suite by automatically updating tests when changes are made to the codebase. It can analyze the impact of code modifications and suggest necessary modifications to the tests.\\n\\nOverall, Langsmith can significantly enhance the testing process by automating various tasks, improving test coverage, generating realistic test data, optimizing and prioritizing tests, and ensuring tests remain up-to-date with code changes.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f6dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4b5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb779ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing by providing a comprehensive set of tools and techniques for automating and streamlining the testing process. Here are some ways in which Langsmith can help:\\n\\n1. Test Case Management: Langsmith offers a test case management system that allows testers to create, organize, and manage test cases effectively. Testers can define test steps, expected results, and track the execution status of each test case.\\n\\n2. Test Automation: Langsmith supports test automation by providing a testing framework that allows testers to write automated test scripts using a variety of programming languages. Testers can easily integrate test scripts with Langsmith's test case management system, enabling them to execute tests automatically and repeatedly.\\n\\n3. Test Data Management: Langsmith offers features to manage test data effectively. Testers can create and manage test data sets, ensuring that the right data is available for each test case.\\n\\n4. Test Execution and Reporting: Langsmith provides a test execution environment that allows testers to execute test cases and track their progress. Testers can generate comprehensive test reports, including test results, coverage, and defect tracking, to gain insights into the overall quality of the software.\\n\\n5. Integration with Continuous Integration/Continuous Delivery (CI/CD) Pipelines: Langsmith integrates seamlessly with CI/CD pipelines, enabling testers to include automated tests as part of the software development process. This integration ensures that tests are executed consistently and continuously, reducing the time and effort required for testing.\\n\\n6. Collaboration and Communication: Langsmith provides collaboration features that facilitate communication and collaboration among testers, developers, and other stakeholders. Testers can share test results, track defects, and discuss testing-related issues, ensuring effective communication and collaboration throughout the testing process.\\n\\nOverall, Langsmith's testing capabilities help streamline the testing process, improve test coverage, and enhance the overall quality of the software being tested.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47a0aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf1a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64baa3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "#openai_api_key= os.environ.get(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d57da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=\"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aaf89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./opt/anaconda3/lib/python3.9/site-packages (1.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd45ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93697f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d05f8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by allowing users to visualize test results.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ef21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3586b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the following features:\n",
      "\n",
      "1. Dataset Upload: LangSmith simplifies the process of uploading datasets, which can be used to test changes to prompts or chains.\n",
      "\n",
      "2. Visualizing Outputs: Once a dataset is uploaded, LangSmith allows running chains over the data points and visualizing the outputs. This helps in manually reviewing the results and identifying any issues or improvements.\n",
      "\n",
      "3. Feedback and Evaluation: LangSmith allows assigning feedback programmatically to runs. This is useful for tracking performance over time and pinpointing underperforming data points. LangSmith also provides evaluators to assess the results of test runs, although these evaluators may not be perfect and should not be blindly trusted.\n",
      "\n",
      "4. Human Evaluation: LangSmith provides annotation queues that allow for manual review and annotation of runs. This is especially helpful for assessing subjective qualities like creativity or humor, and validating the results of automatic evaluation metrics.\n",
      "\n",
      "Overall, LangSmith provides tools and functionalities to facilitate the testing process and improve the quality and reliability of LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa2700",
   "metadata": {},
   "source": [
    "the response from the LLM is seen above which is more accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f0705ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2278ee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2abde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1d6191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith can help test your LLM applications in several ways:\\n\\n1. Dataset Creation: LangSmith makes it easy to curate datasets for testing. You can create datasets by collecting examples during the development process or by manually constructing a small dataset. These datasets can be exported for use in other contexts, such as OpenAI Evals or fine-tuning.\\n\\n2. Testing Prompt and Chain Changes: Once you have a dataset, you can use it to test changes to prompts or chains. You can run the chain over the data points and visualize the outputs. The LangSmith client simplifies this process by allowing you to pull down a dataset and run a chain over them, logging the results to a new project associated with the dataset. You can review these results and assign feedback to runs, marking them as correct or incorrect. Aggregate statistics for each test project are displayed to help you evaluate the changes.\\n\\n3. Evaluators and Human Evaluation: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While automatic evaluation metrics are helpful, human review is crucial for quality and reliability. LangSmith makes it easy to manually review and annotate runs through annotation queues. These queues allow you to select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. Reviewers can step through the runs, view the input, output, and existing tags, and add their own feedback. This manual review helps assess subjective qualities and validate auto-evaluated runs.\\n\\n4. Monitoring and Troubleshooting: LangSmith can also be used to monitor your LLM applications in production. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Feedback can be programmatically associated with runs, allowing you to track performance over time and pinpoint underperforming data points. This information can be used to improve your models and add them to datasets for future testing.\\n\\nOverall, LangSmith provides a comprehensive testing and evaluation framework for LLM applications, enabling you to ensure their reliability and quality.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58cee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff505f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in ./opt/anaconda3/lib/python3.9/site-packages (0.0.13)\n",
      "Requirement already satisfied: tavily-python in ./opt/anaconda3/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (0.6.3)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (0.1.11)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (0.0.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (1.21.5)\n",
      "Requirement already satisfied: requests<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (8.2.3)\n",
      "Collecting tiktoken==0.5.1 (from tavily-python)\n",
      "  Using cached tiktoken-0.5.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./opt/anaconda3/lib/python3.9/site-packages (from tiktoken==0.5.1->tavily-python) (2022.7.9)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain-community) (3.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain-community) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain-community) (1.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2022.9.24)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.9->langchain-community) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.9->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Using cached tiktoken-0.5.1-cp39-cp39-macosx_10_9_x86_64.whl (953 kB)\n",
      "Installing collected packages: tiktoken\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.2\n",
      "    Uninstalling tiktoken-0.5.2:\n",
      "      Successfully uninstalled tiktoken-0.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.0.2.post1 requires tiktoken<0.6.0,>=0.5.2, but you have tiktoken 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc28f42a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c4a5b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n__root__\n  Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7b/0wgy81n144n5r2nf87nln_5h0000gp/T/ipykernel_99269/1529022514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTavilySearchResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtavily_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pydantic/main.cpython-39-darwin.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pydantic/main.cpython-39-darwin.so\u001b[0m in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pydantic/fields.cpython-39-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.get_default\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pydantic/main.cpython-39-darwin.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n__root__\n  Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "#search = TavilySearchResults(tavily_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62ccd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools =[retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0f0b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchainhub) (2.28.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (2022.9.24)\n",
      "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/types-urllib3/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-urllib3, types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.6 types-urllib3-1.26.25.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9a4ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=\"....\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af810df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n",
      "\n",
      "1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the system under test. It uses techniques like symbolic execution and constraint solving to generate test inputs that cover different paths and conditions in the code.\n",
      "\n",
      "2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the system under test for a given set of inputs. It can assist in specifying the expected results and behaviors in a formal and precise manner.\n",
      "\n",
      "3. Test Execution: LangSmith can execute the generated test cases and provide feedback on the results. It can detect failures and deviations from the expected behavior, helping in identifying bugs and issues in the system under test.\n",
      "\n",
      "4. Test Coverage Analysis: LangSmith can analyze the coverage of the test cases, i.e., how much of the code or functionality is exercised by the tests. It can provide insights into the areas of the system that are not adequately covered by the tests, helping in improving the test suite.\n",
      "\n",
      "5. Test Prioritization: LangSmith can prioritize the execution of test cases based on various criteria, such as code coverage, fault detection capability, and resource constraints. It can help in optimizing the testing process by executing the most critical and effective test cases first.\n",
      "\n",
      "Overall, LangSmith can assist in automating and improving the testing process, making it more efficient, effective, and reliable.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith can help with testing in several ways:\\n\\n1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the system under test. It uses techniques like symbolic execution and constraint solving to generate test inputs that cover different paths and conditions in the code.\\n\\n2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the system under test for a given set of inputs. It can assist in specifying the expected results and behaviors in a formal and precise manner.\\n\\n3. Test Execution: LangSmith can execute the generated test cases and provide feedback on the results. It can detect failures and deviations from the expected behavior, helping in identifying bugs and issues in the system under test.\\n\\n4. Test Coverage Analysis: LangSmith can analyze the coverage of the test cases, i.e., how much of the code or functionality is exercised by the tests. It can provide insights into the areas of the system that are not adequately covered by the tests, helping in improving the test suite.\\n\\n5. Test Prioritization: LangSmith can prioritize the execution of test cases based on various criteria, such as code coverage, fault detection capability, and resource constraints. It can help in optimizing the testing process by executing the most critical and effective test cases first.\\n\\nOverall, LangSmith can assist in automating and improving the testing process, making it more efficient, effective, and reliable.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13ee97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'weather in San Francisco'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content=\"types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?â€‹If a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?â€‹Building and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debuggingâ€‹In the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a â€œShareâ€� button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examplesâ€‹Most of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, itâ€™s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You donâ€™t have any examples to benchmark your changes against.LangSmith addresses this problem by including an â€œAdd to Datasetâ€� button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluationâ€‹Initially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]\u001b[0m\u001b[32;1m\u001b[1;3mI'm sorry, but I couldn't find the current weather in San Francisco.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in SF?',\n",
       " 'output': \"I'm sorry, but I couldn't find the current weather in San Francisco.\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "207172ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help test your LLM applications by providing valuable feedback and suggestions to improve your application. Here's how it works:\n",
      "\n",
      "1. Personal Statement Review: LangSmith can review your personal statement and provide feedback on the content, structure, and overall effectiveness. It can suggest improvements to make your statement more compelling and persuasive.\n",
      "\n",
      "2. Resume/CV Review: LangSmith can review your resume or CV and provide suggestions on how to highlight your relevant experiences, skills, and achievements. It can help you present your qualifications in the most effective way.\n",
      "\n",
      "3. Recommendation Letter Guidance: LangSmith can provide guidance on selecting the right recommenders and offer tips on how to request strong recommendation letters. It can also review draft letters and provide feedback to ensure they are impactful and supportive of your application.\n",
      "\n",
      "4. Application Strategy: LangSmith can help you develop a comprehensive application strategy, including selecting the right law schools, understanding their specific requirements, and planning your timeline for submission.\n",
      "\n",
      "5. Interview Preparation: LangSmith can provide guidance and practice for your LLM interviews. It can help you anticipate common interview questions and provide tips on how to effectively communicate your motivations and qualifications.\n",
      "\n",
      "6. Scholarship and Financial Aid Assistance: LangSmith can provide information and guidance on available scholarships and financial aid options for LLM programs. It can help you navigate the application process and increase your chances of securing funding.\n",
      "\n",
      "To access these services, you can reach out to LangSmith and provide them with the necessary documents and information. They will then review your materials and provide you with personalized feedback and guidance to enhance your LLM applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'output': \"LangSmith can help test your LLM applications by providing valuable feedback and suggestions to improve your application. Here's how it works:\\n\\n1. Personal Statement Review: LangSmith can review your personal statement and provide feedback on the content, structure, and overall effectiveness. It can suggest improvements to make your statement more compelling and persuasive.\\n\\n2. Resume/CV Review: LangSmith can review your resume or CV and provide suggestions on how to highlight your relevant experiences, skills, and achievements. It can help you present your qualifications in the most effective way.\\n\\n3. Recommendation Letter Guidance: LangSmith can provide guidance on selecting the right recommenders and offer tips on how to request strong recommendation letters. It can also review draft letters and provide feedback to ensure they are impactful and supportive of your application.\\n\\n4. Application Strategy: LangSmith can help you develop a comprehensive application strategy, including selecting the right law schools, understanding their specific requirements, and planning your timeline for submission.\\n\\n5. Interview Preparation: LangSmith can provide guidance and practice for your LLM interviews. It can help you anticipate common interview questions and provide tips on how to effectively communicate your motivations and qualifications.\\n\\n6. Scholarship and Financial Aid Assistance: LangSmith can provide information and guidance on available scholarships and financial aid options for LLM programs. It can help you navigate the application process and increase your chances of securing funding.\\n\\nTo access these services, you can reach out to LangSmith and provide them with the necessary documents and information. They will then review your materials and provide you with personalized feedback and guidance to enhance your LLM applications.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "407646c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langserve[all] in ./opt/anaconda3/lib/python3.9/site-packages (0.0.39)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (0.26.0)\n",
      "Requirement already satisfied: langchain>=0.0.333 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (0.1.1)\n",
      "Requirement already satisfied: orjson>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (3.9.10)\n",
      "Requirement already satisfied: pydantic>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (1.10.8)\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (0.109.0)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (0.4.0)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langserve[all]) (1.8.2)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in ./opt/anaconda3/lib/python3.9/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./opt/anaconda3/lib/python3.9/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (4.9.0)\n",
      "Requirement already satisfied: anyio in ./opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.23.0->langserve[all]) (3.5.0)\n",
      "Requirement already satisfied: certifi in ./opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.23.0->langserve[all]) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in ./opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.23.0->langserve[all]) (1.0.2)\n",
      "Requirement already satisfied: idna in ./opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.23.0->langserve[all]) (3.3)\n",
      "Requirement already satisfied: sniffio in ./opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.23.0->langserve[all]) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.23.0->langserve[all]) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.13)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (0.1.11)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (1.21.5)\n",
      "Requirement already satisfied: requests<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.333->langserve[all]) (8.2.3)\n",
      "Requirement already satisfied: uvicorn in ./opt/anaconda3/lib/python3.9/site-packages (from sse-starlette<2.0.0,>=1.3.0->langserve[all]) (0.23.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./opt/anaconda3/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.9->langchain>=0.0.333->langserve[all]) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (1.26.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.333->langserve[all]) (1.1.1)\n",
      "Requirement already satisfied: click>=7.0 in ./opt/anaconda3/lib/python3.9/site-packages (from uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (8.0.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcffd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
